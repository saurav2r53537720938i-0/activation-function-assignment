{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffcc32e-0c3c-4f83-b287-952751962d35",
   "metadata": {},
   "source": [
    "In the realm of artficial neural networks,an activation function seves as a mathemtical application applied to the output of the introduce the linearity neural works to learn compares and relationship in data common activation function include tygomid tanh Relu and softmax.Each function has its own chartertics suited for diffrent types of tasks and network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345fcd54-edfb-4812-afe2-ccb4de371235",
   "metadata": {},
   "source": [
    "Some common types of activation funtion used in neural networks include:\n",
    "1. Sigmoid\n",
    "2. Tanh\n",
    "3. Rectfied linear unit\n",
    "4. Leaky RELU\n",
    "5. Softmax\n",
    "These activation function each have unique properties and are chosen based on the specfic requirments of netural network and the task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd51c72-d656-4738-a278-21037c3c51d6",
   "metadata": {},
   "source": [
    "Activation function play a cruicial role in the traninng process and performance of a neural network in sevral ways:\n",
    "1. Non-lineaerity\n",
    "2. Gradient flow\n",
    "3. Vanishing gradient\n",
    "4. Sparsity\n",
    "5. Saturation\n",
    "overall the choice of activation function influence their the networks capcity influence the networks capcity to learn complex reprsentations the stablity of traning and its ablity to genrlize to unseen data Experiment and careful slection of activation function are essential for designing effective neural network architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89edf229-4197-449c-8648-9988ae8a283d",
   "metadata": {},
   "source": [
    "The sigmoid activation function also known as the logistic function works by squashing the input values into a range between 0and 1 mathemticallyy is defined as\n",
    "1. Output range\n",
    "2. Differnetiablity\n",
    "3. Vanishing gradient\n",
    "4. Not zero-centred\n",
    "5. Satauration\n",
    "Due to these disadvantage sigmoid activation functyions are less commonly used of function are less commonly and its variants however they are still in output layers for binary classfication tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158196bd-4963-4774-ad5b-c8e9ec32d2a8",
   "metadata": {},
   "source": [
    "The recfied linear unit activation function is a piecewise linear function that returns zero for negative inputs tha returns zero for negative input and the input valueitself for postive inputs :\n",
    "1. Linearity\n",
    "2. Sparsity\n",
    "3. Gradient Vanishing\n",
    "4. Output range\n",
    "overall Relu has become the default choice for activation functions in many neural networks simplicity efficency and effectivness in traning deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02fb8c8-c6e9-4680-8d90-509f4cc539f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
